{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4285e981",
   "metadata": {},
   "source": [
    "# <center> <b> <span style=\"color: hotpink;\">  ShadeSense </center> #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337b4fe",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "\n",
    "![](https://github.com/ginaguerin/ShadeSense_Lipstick_Shade_Identifier_App/blob/master/logos/logo3.2.jpeg?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162e442",
   "metadata": {},
   "source": [
    "## <u> Concept: </u> ##\n",
    "\n",
    "- ShadeSense is an innovative application designed to instantly identify the specific shade of lipstick someone is wearing in real-time. The ultimate goal of this application is to create an extensive library capable of recognizing a wide range of lipstick shades and brands. Importantly, ShadeSense aims to be inclusive, ensuring accurate detection across all lipstick colors, irrespective of gender or skin tone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c371f",
   "metadata": {},
   "source": [
    "## <u> Scope: </u> ##\n",
    "\n",
    "- In its initial phase, ShadeSense will begin with a curated selection from the renowned makeup brand, MAC Cosmetics. This curated collection consists of five distinct lipstick shades, serving as a starting point for the app's development. As TwinTone evolves, it will expand its library to encompass a diverse array of lipstick shades from various brands, further enhancing its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aab1d8",
   "metadata": {},
   "source": [
    "## Dataset Overview: ##\n",
    "\n",
    "- **Images:** The dataset comprises original resized images in JPG format. These images are the input data for training the lipstick shade identification model.\n",
    "\n",
    "- **Annotations:** Each image is associated with an annotated XML file. These XML files likely contain information about the location and class labels of the lipstick shades within the corresponding image.\n",
    "\n",
    "- **Classes:** There are 7 distinct classes corresponding to different lipstick shades. The goal is to train the model to recognize and classify these lipstick shades automatically.\n",
    "\n",
    "- **Supervised Learning:** The dataset is suitable for supervised learning, where the model learns from the paired examples of images and their corresponding annotations to generalize and make predictions on new, unseen data.\n",
    "\n",
    "- **Training Objective:** The objective of the model is to analyze the images and identify the correct lipstick shade class based on the provided annotations.\n",
    "\n",
    "- **Neural Network Architecture:** The CNN architecture outlined in the baseline model is designed to process the image data and make predictions for the 7 lipstick shade classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db06927",
   "metadata": {},
   "source": [
    "## <u> <span style=\"color: red;\"> Limitations: </span> </u> ##\n",
    "\n",
    "\n",
    "1. **Limited Dataset Size:**\n",
    "   - With only 220 images for the six lipstick shades, including images of no lipstick, the dataset might be relatively small for training a highly accurate and robust neural network.\n",
    "2. **Model Generalization:**\n",
    "   - The model's ability to generalize to different lipstick shades, brands, and skin tones might be limited initially. Training on a diverse dataset can help improve generalization.\n",
    "\n",
    "3. **Brand and Shade Specificity:**\n",
    "   - The initial focus on MAC Cosmetics and six specific shades may limit the app's applicability to users with different preferences or those using other brands. Expanding the dataset to include various brands and shades can address this limitation over time.\n",
    "\n",
    "4. **Real-Time Processing Constraints:**\n",
    "   - Real-time processing on live camera feeds can be computationally intensive, leading to potential performance limitations, especially on devices with lower processing power. Optimizing the model and deploying it on platforms that support efficient real-time processing.\n",
    "\n",
    "5. **Lighting and Environmental Conditions:**\n",
    "   - The accuracy of the lipstick detection may be influenced by lighting conditions and the environment. Variations in lighting may impact color perception, potentially affecting the model's performance.\n",
    "\n",
    "6. **User Privacy Concerns:**\n",
    "   - The app involves capturing and processing images in real-time, raising privacy concerns. Making sure to clearly communicate to users how their data will be handled, stored, and if any images are stored temporarily for processing.\n",
    "\n",
    "7. **Device Compatibility:**\n",
    "   - The app's real-time detection capabilities may be influenced by the camera quality and specifications of the user's device. Ensure compatibility across a range of devices and optimize the app's performance accordingly.\n",
    "\n",
    "8. **Legal and Ethical Considerations:**\n",
    "   - Ensure compliance with legal and ethical standards, especially when dealing with image data. Being aware of privacy laws, data protection regulations, and obtain consent when necessary.\n",
    "\n",
    "9. **Feedback and Iterative Development:**\n",
    "   - Given the initial focus on a smaller dataset and makeup brand, user feedback will be crucial for identifying limitations and areas for improvement. Planning for iterative development to enhance the app based on user experiences and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9ec27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 11:27:46.401887: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ea8f3",
   "metadata": {},
   "source": [
    "### Image Data Preprocessing ###\n",
    "\n",
    "\n",
    "1. **File Discovery**\n",
    "   \n",
    "\n",
    "2. **Annotation Pairing**\n",
    "\n",
    "\n",
    "3. **Class Extraction**\n",
    "   \n",
    "\n",
    "4. **Image Loading and Conversion**\n",
    "   \n",
    "\n",
    "5. **Final Preprocessing**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12cfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"images\"\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "all_files = os.listdir(folder_path)\n",
    "\n",
    "# Filter only image files\n",
    "image_files = [f for f in all_files if f.lower().endswith(\".jpg\")]\n",
    "\n",
    "# Create a DataFrame with columns \"filename\" and \"annotation\"\n",
    "df = pd.DataFrame({'filename': image_files})\n",
    "\n",
    "# Corresponding XML file with the same name\n",
    "df['annotation'] = df['filename'].apply(lambda x: os.path.splitext(x)[0] + \".xml\")\n",
    "\n",
    "# Extract class name from XML\n",
    "def extract_class_name(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(os.path.join(folder_path, xml_path))\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        \n",
    "        object_name_element = root.find('.//object/name')\n",
    "        class_name = object_name_element.text if object_name_element is not None else None\n",
    "        \n",
    "        return class_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting class name from {xml_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create a new column \"class_name\"\n",
    "df['class_name'] = df['annotation'].apply(extract_class_name)\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_and_preprocess_image(image_path, target_size=(512, 512)):\n",
    "    try:\n",
    "        # Load the image and resize it\n",
    "        image = Image.open(os.path.join(folder_path, image_path))\n",
    "        image = image.resize(target_size)\n",
    "        \n",
    "        # Convert the image to a NumPy array\n",
    "        image_data = np.array(image)\n",
    "        \n",
    "        # Convert the NumPy array to a TensorFlow tensor\n",
    "        image_tensor = tf.convert_to_tensor(image_data, dtype=tf.float32)\n",
    "        \n",
    "        return image_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Create a new column \"image_data\"\n",
    "df['image_data'] = df['filename'].apply(load_and_preprocess_image)\n",
    "\n",
    "\n",
    "# Preprocess image data\n",
    "def preprocess_image(image_data, target_size=(512, 512)):\n",
    "    try:\n",
    "        # Convert the NumPy array to a TensorFlow tensor\n",
    "        image = tf.convert_to_tensor(image_data, dtype=tf.float32)\n",
    "        \n",
    "        # Resize and preprocess the image\n",
    "        image = tf.image.resize(image, target_size)\n",
    "        image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Preprocessing to image data\n",
    "df['image'] = df['image_data'].apply(preprocess_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9378d",
   "metadata": {},
   "source": [
    "### Data Splitting ###\n",
    "\n",
    "\n",
    "1. **Dataset Splitting:**\n",
    "   - The `train_test_split` function is utilized to partition the dataset into training and test sets.\n",
    "\n",
    "2. **Set Size Information:**\n",
    "   - The code prints the size of both the training and test sets, providing insights into the distribution of data between the two subsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3062c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 176\n",
      "Test set size: 44\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the number of items in each set\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160bfa1",
   "metadata": {},
   "source": [
    "### Data Preprocessing ###\n",
    "\n",
    "**Handling Missing Labels**\n",
    "   \n",
    "\n",
    "2. **Label Encoding**\n",
    "   \n",
    "\n",
    "3. **Image Normalization**\n",
    "   \n",
    "\n",
    "4. **Remaining Items Information**\n",
    "   \n",
    "\n",
    "5. **Unique Labels Verification**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd2f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items remaining in the training set: 175\n",
      "Number of items remaining in the test set: 44\n",
      "Unique labels in training set: [0 1 2 3 4 5 6]\n",
      "Unique labels in the test set: [0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Handle missing labels by dropping rows with missing labels\n",
    "train_df = train_df.dropna(subset=['class_name'])\n",
    "test_df = test_df.dropna(subset=['class_name'])\n",
    "\n",
    "# Reapply the encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(train_df['class_name'])\n",
    "encoded_labels_test = label_encoder.transform(test_df['class_name'])\n",
    "\n",
    "# Normalize images in the training set\n",
    "train_images = np.stack(train_df['image'].to_numpy())\n",
    "train_images_normalized = train_images / 255.0 \n",
    "\n",
    "# Normalize images in the test set using the same parameters as the training set\n",
    "test_images = np.stack(test_df['image'].to_numpy())\n",
    "test_images_normalized = test_images / 255.0 \n",
    "\n",
    "\n",
    "# Print the number of items remaining after dropping rows\n",
    "print(\"Number of items remaining in the training set:\", len(train_df))\n",
    "print(\"Number of items remaining in the test set:\", len(test_df))\n",
    "\n",
    "# Print unique labels for verification\n",
    "print(\"Unique labels in training set:\", np.unique(encoded_labels_train))\n",
    "print(\"Unique labels in the test set:\", np.unique(encoded_labels_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad1a05",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965cbf9",
   "metadata": {},
   "source": [
    "The baseline model is meticulously crafted for the lipstick shade identification task using a dataset comprising original resized JPG images and their corresponding annotated XML files. Here are the key aspects of the baseline model, with a focus on monitoring the F-1 score due to our small dataset, imbalanced classes, and concerns for misclassifying categories:\n",
    "\n",
    "- **Model Architecture:** A Convolutional Neural Network (CNN) is tailored for image processing, featuring three convolutional layers (32, 64, and 128 filters), subsequent max-pooling layers, a flatten layer, and two dense layers. The model is specifically designed to process images with dimensions (512, 512, 3).\n",
    "\n",
    "- **Dataset Structure:** Our dataset consists of paired examples, linking original resized images to annotated XML files. Each image is enriched with details about the location and class labels of various lipstick shades.\n",
    "\n",
    "- **Supervised Learning:** The model adopts a supervised learning approach, learning to make predictions based on annotated images, with the objective of automatically categorizing lipstick shades into 7 distinct classes.\n",
    "\n",
    "- **Training Objective:** The primary aim is to train the model for accurate recognition and classification of lipstick shades. Ground truth information from annotated XML files serves as the foundation for model training.\n",
    "\n",
    "- **Model Compilation:** The model is compiled using the Adam optimizer, sparse categorical crossentropy loss function, and includes F-1 score as an additional evaluation metric. This modification aligns with our focus on addressing the challenges posed by our small dataset and imbalanced classes.\n",
    "\n",
    "- **Training Process:** The model undergoes training for 10 epochs using the paired dataset, optimizing parameters to minimize crossentropy loss and maximize the F-1 score. The validation dataset plays a crucial role in evaluating the model's generalization to unseen data.\n",
    "\n",
    "This baseline model acts as a pivotal step in constructing a robust lipstick shade identification system. It lays the groundwork for subsequent enhancements and tuning efforts, ensuring our model evolves and performs optimally on our specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e3cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 119s 18s/step - loss: 5.7034 - accuracy: 0.1600 - val_loss: 1.8986 - val_accuracy: 0.3864\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 82s 13s/step - loss: 1.8783 - accuracy: 0.3371 - val_loss: 1.8532 - val_accuracy: 0.3409\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 85s 14s/step - loss: 1.5384 - accuracy: 0.5143 - val_loss: 1.5259 - val_accuracy: 0.3864\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 82s 13s/step - loss: 1.0088 - accuracy: 0.6800 - val_loss: 1.3059 - val_accuracy: 0.5455\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 86s 14s/step - loss: 0.4990 - accuracy: 0.8343 - val_loss: 1.1816 - val_accuracy: 0.7273\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 74s 12s/step - loss: 0.2387 - accuracy: 0.9543 - val_loss: 0.7935 - val_accuracy: 0.7727\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 86s 14s/step - loss: 0.0848 - accuracy: 0.9829 - val_loss: 0.7612 - val_accuracy: 0.7727\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 97s 16s/step - loss: 0.0636 - accuracy: 0.9657 - val_loss: 0.5135 - val_accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 98s 15s/step - loss: 0.0360 - accuracy: 0.9943 - val_loss: 1.1855 - val_accuracy: 0.7045\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 66s 10s/step - loss: 0.0174 - accuracy: 0.9943 - val_loss: 0.8105 - val_accuracy: 0.7500\n",
      "Test Loss: 0.8105, Test Accuracy: 0.7500\n",
      "2/2 [==============================] - 3s 868ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Crème D'Nude       0.50      1.00      0.67         3\n",
      "     Honey Love       0.75      0.90      0.82        10\n",
      "Lasting Passion       0.56      0.83      0.67         6\n",
      "           None       1.00      1.00      1.00         1\n",
      "       Ruby Woo       0.80      1.00      0.89         4\n",
      "          Stone       1.00      0.62      0.77         8\n",
      "          Whirl       1.00      0.50      0.67        12\n",
      "\n",
      "       accuracy                           0.75        44\n",
      "      macro avg       0.80      0.84      0.78        44\n",
      "   weighted avg       0.83      0.75      0.75        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model with normalized input\n",
    "num_classes = len(np.unique(encoded_labels_train))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with normalized training set\n",
    "history = model.fit(\n",
    "    train_images_normalized,\n",
    "    encoded_labels_train,\n",
    "    epochs=10,\n",
    "    validation_data=(test_images_normalized, encoded_labels_test)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_images_normalized, encoded_labels_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(test_images_normalized)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert encoded labels back to original labels\n",
    "original_labels_test = label_encoder.inverse_transform(encoded_labels_test)\n",
    "predicted_labels_test = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(original_labels_test, predicted_labels_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d829c",
   "metadata": {},
   "source": [
    "### Observations from Baseline Model: ###\n",
    "\n",
    "Upon reviewing the baseline model, certain observations and key metrics were identified, highlighting aspects of overfitting and performance during training:\n",
    "\n",
    "- **Epoch 1/10:**\n",
    "  - Training Loss: 5.7034, Training Accuracy: 16%\n",
    "  - Validation Loss: 1.8986, Validation Accuracy: 39%\n",
    "\n",
    "- **Epoch 10/10:**\n",
    "  - Training Loss: 0.0174, Training Accuracy: 99%\n",
    "  - Validation Loss: 0.8105, Validation Accuracy: 75%\n",
    "\n",
    "### Anticipated Improvements with Enhanced Model: ###\n",
    "For an enhanced model, specific strategies were proposed to address overfitting and enhance overall performance:\n",
    "\n",
    "1. **Overfitting Mitigation:**\n",
    "   - The introduction of L2 regularization and dropout layers is anticipated to alleviate overfitting concerns.\n",
    "\n",
    "2. **Balanced Accuracy:**\n",
    "   - Expectations are set for improved balance in accuracy across both training and validation sets.\n",
    "\n",
    "\n",
    "3. **Challenges and Opportunities:**\n",
    "   - The model performs well on certain classes but encounters challenges, particularly with \"Whirl,\" where precision and recall are lower.The imbalanced distribution of classes, especially the smaller classes, can influence the model's ability to generalize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf438fcd",
   "metadata": {},
   "source": [
    "\n",
    "## Anticipating Enhanced Model Performance: ##\n",
    "\n",
    "- **Model Type:** Convolutional Neural Network (CNN)\n",
    "- **Enhancements:**\n",
    "  - L2 Regularization: Integrated with a weight decay of 0.01\n",
    "  - Dropout Layers: Strategically placed with dropout rates of 0.25 and 0.5\n",
    "- **Objective:** Fortify against overfitting and enhance adaptability\n",
    "\n",
    "### Evaluation Metrics Outlook: ###\n",
    "Anticipated evaluation metrics for the enhanced model:\n",
    "\n",
    "- **Test Loss: 1.2101, Test Accuracy: 77%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e06ddc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 105s 14s/step - loss: 3419.6638 - accuracy: 0.1429 - val_loss: 32.6378 - val_accuracy: 0.1591\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 99s 16s/step - loss: 74.9371 - accuracy: 0.1486 - val_loss: 15.0598 - val_accuracy: 0.2045\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 83s 13s/step - loss: 16.4527 - accuracy: 0.2171 - val_loss: 18.2549 - val_accuracy: 0.3409\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 98s 15s/step - loss: 18.9322 - accuracy: 0.2914 - val_loss: 20.1825 - val_accuracy: 0.4091\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 82s 14s/step - loss: 20.4786 - accuracy: 0.3257 - val_loss: 21.2733 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 78s 12s/step - loss: 21.0242 - accuracy: 0.4971 - val_loss: 21.6287 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 84s 14s/step - loss: 21.0958 - accuracy: 0.6229 - val_loss: 21.9288 - val_accuracy: 0.4773\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 80s 13s/step - loss: 20.9557 - accuracy: 0.8057 - val_loss: 21.5489 - val_accuracy: 0.5682\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 72s 12s/step - loss: 20.7191 - accuracy: 0.8457 - val_loss: 21.4714 - val_accuracy: 0.6818\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 57s 9s/step - loss: 20.5985 - accuracy: 0.9029 - val_loss: 21.3792 - val_accuracy: 0.6818\n",
      "Test Loss: 21.3792, Test Accuracy: 0.6818\n",
      "2/2 [==============================] - 3s 701ms/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Crème D'Nude       0.60      1.00      0.75         3\n",
      "     Honey Love       0.80      0.40      0.53        10\n",
      "Lasting Passion       0.46      1.00      0.63         6\n",
      "           None       1.00      1.00      1.00         1\n",
      "       Ruby Woo       0.75      0.75      0.75         4\n",
      "          Stone       1.00      0.62      0.77         8\n",
      "          Whirl       0.73      0.67      0.70        12\n",
      "\n",
      "       accuracy                           0.68        44\n",
      "      macro avg       0.76      0.78      0.73        44\n",
      "   weighted avg       0.76      0.68      0.68        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and fit label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(train_df['class_name'])\n",
    "encoded_labels_test = label_encoder.transform(test_df['class_name'])\n",
    "\n",
    "# Define the CNN model with regularization and dropout\n",
    "num_classes = len(np.unique(encoded_labels_train))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3), kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25), \n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),  \n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with the Adam optimizer\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    np.stack(train_df['image'].to_numpy()),\n",
    "    encoded_labels_train,\n",
    "    epochs=10,\n",
    "    validation_data=(np.stack(test_df['image'].to_numpy()), encoded_labels_test)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(np.stack(test_df['image'].to_numpy()), encoded_labels_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(np.stack(test_df['image'].to_numpy()))\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert encoded labels back to original labels\n",
    "original_labels_test = label_encoder.inverse_transform(encoded_labels_test)\n",
    "predicted_labels_test = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(original_labels_test, predicted_labels_test, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033a752",
   "metadata": {},
   "source": [
    "## Summary of Enhanced Model Performance ##\n",
    "\n",
    "### Model Architecture Enhancements: ###\n",
    "The enhanced model introduced several architectural modifications, including dropout layers and L2 regularization, to address overfitting concerns. However, the model's performance in terms of accuracy and key metrics did not show significant improvement.\n",
    "\n",
    "### Training and Evaluation Results: ###\n",
    "The training and validation results over 10 epochs for the enhanced model are outlined below:\n",
    "\n",
    "- **Epoch 1/10:**\n",
    "  - Training Loss: 3419.66, Training Accuracy: 14.29%\n",
    "  - Validation Loss: 32.64, Validation Accuracy: 15.91%\n",
    "\n",
    "- **Epoch 10/10:**\n",
    "  - Training Loss: 20.5985, Training Accuracy: 90.29%\n",
    "  - Validation Loss: 21.3792, Validation Accuracy: 68.18%\n",
    "\n",
    "### Performance Metrics: ###\n",
    "The classification report provides insights into the model's capability to classify lipstick shade categories. Key observations include:\n",
    "\n",
    "- **Classification Report:**\n",
    "  - \"Crème D'Nude\" and \"None\" show high precision, recall, and F-1 scores, indicating strong performance. \n",
    "  - \"Honey Love\" and \"Whirl\" face challenges, with lower F-1 scores, suggesting potential areas for improvement.\n",
    "  - Overall accuracy is 68.18%, indicating some improvements in correctly classifying lipstick shades.\n",
    "\n",
    "### Observations and Next Steps: ###\n",
    "1. Despite architectural enhancements, the model's performance remains suboptimal, with limited improvements in accuracy.\n",
    "2. Further exploration of hyperparameters, model complexity, and potential consideration of alternative architectures (e.g., deeper networks or transfer learning) could be pivotal in addressing the observed limitations.\n",
    "3. Continue model refinement and fine-tuning are recommended to achieve more desirable classification outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d7f72",
   "metadata": {},
   "source": [
    "## Model Tuning for Improved Performance ##\n",
    "\n",
    "- **Model Type:** Convolutional Neural Network (CNN) based on VGG16\n",
    "- **Enhancements:**\n",
    "  - L2 Regularization: Applied with a weight decay of 0.0001\n",
    "  - Dropout Layers: Strategically placed with a dropout rate of 0.1\n",
    "  - Learning Rate: Adjusted to 0.0001\n",
    "- **Epoch Cycle:** Set to 10 epochs\n",
    "\n",
    "Our strategy for model enhancement involves leveraging L2 regularization, dropout layers, and a custom learning rate. Additionally, data augmentation is applied to increase the model's robustness and generalization capability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "367617e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 335s 48s/step - loss: 9.1102 - accuracy: 0.1886 - val_loss: 3.4273 - val_accuracy: 0.3864\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 410s 59s/step - loss: 2.2114 - accuracy: 0.2914 - val_loss: 1.8295 - val_accuracy: 0.2727\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 401s 58s/step - loss: 1.6334 - accuracy: 0.3829 - val_loss: 1.6145 - val_accuracy: 0.3636\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 388s 56s/step - loss: 1.5214 - accuracy: 0.4457 - val_loss: 1.6219 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 372s 52s/step - loss: 1.2882 - accuracy: 0.5714 - val_loss: 1.4234 - val_accuracy: 0.5227\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 372s 53s/step - loss: 1.0194 - accuracy: 0.6571 - val_loss: 1.1056 - val_accuracy: 0.6364\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 376s 61s/step - loss: 0.9671 - accuracy: 0.6914 - val_loss: 0.8625 - val_accuracy: 0.7273\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 319s 46s/step - loss: 0.6513 - accuracy: 0.7886 - val_loss: 0.6681 - val_accuracy: 0.7727\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 361s 52s/step - loss: 0.5360 - accuracy: 0.8057 - val_loss: 0.6609 - val_accuracy: 0.7727\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 373s 60s/step - loss: 0.4979 - accuracy: 0.8571 - val_loss: 0.4315 - val_accuracy: 0.8182\n",
      "Test Loss: 0.4315, Test Accuracy: 0.8182\n",
      "2/2 [==============================] - 71s 16s/step\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Crème D'Nude       1.00      1.00      1.00         3\n",
      "     Honey Love       1.00      0.40      0.57        10\n",
      "Lasting Passion       0.86      1.00      0.92         6\n",
      "           None       1.00      1.00      1.00         1\n",
      "       Ruby Woo       1.00      0.75      0.86         4\n",
      "          Stone       0.53      1.00      0.70         8\n",
      "          Whirl       1.00      0.92      0.96        12\n",
      "\n",
      "       accuracy                           0.82        44\n",
      "      macro avg       0.91      0.87      0.86        44\n",
      "   weighted avg       0.90      0.82      0.81        44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "# Create and fit label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels_train = label_encoder.fit_transform(train_df['class_name'])\n",
    "encoded_labels_test = label_encoder.transform(test_df['class_name'])\n",
    "\n",
    "# Define the CNN model with regularization and dropout\n",
    "num_classes = len(np.unique(encoded_labels_train))\n",
    "\n",
    "# Load the VGG16 model without the top (fully connected) layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "\n",
    "# Freeze the convolutional layers of VGG16\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model and add the VGG16 base model with dropout\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(186, activation='relu', kernel_regularizer=regularizers.l2(0.0001)),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Unfreeze the last 3 layers of the base model\n",
    "for layer in base_model.layers[-3:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model with the custom optimizer (Adam with a learning rate of 0.0001)\n",
    "custom_optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "# Assuming you have a custom optimizer named custom_optimizer\n",
    "model.compile(optimizer=custom_optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy', metrics.Precision(), metrics.Recall(), metrics.F1Score()])\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    width_shift_range=0.1,  \n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1, \n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(\n",
    "    datagen.flow(np.stack(train_df['image'].to_numpy()), encoded_labels_train, batch_size=28),\n",
    "    epochs=10,\n",
    "    validation_data=(np.stack(test_df['image'].to_numpy()), encoded_labels_test)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(np.stack(test_df['image'].to_numpy()), encoded_labels_test, verbose=0)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(np.stack(test_df['image'].to_numpy()))\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert encoded labels back to original labels\n",
    "original_labels_test = label_encoder.inverse_transform(encoded_labels_test)\n",
    "predicted_labels_test = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(original_labels_test, predicted_labels_test, zero_division=1))\n",
    "\n",
    "# Save the label encoder classes\n",
    "np.save('models/label_encoder_classes2.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24fb1f",
   "metadata": {},
   "source": [
    "## Summary of Final Model with Lowered Dropout Rate ##\n",
    "\n",
    "### Model Architecture and Training Dynamics: ###\n",
    "The model, incorporating VGG16 with a lowered dropout rate of 0.38, demonstrates improved stability and generalization during an 8-epoch training regimen:\n",
    "\n",
    "## Interpretation of Model Results ##\n",
    "\n",
    "- **Epoch 1/8:**\n",
    "  - Training Loss: 9.11, Training Accuracy: 18.86%\n",
    "  - Validation Loss: 3.43, Validation Accuracy: 38.64%\n",
    "\n",
    "- **Epoch 8/8:**\n",
    "  - Training Loss: 0.43, Training Accuracy: 81.82%\n",
    "  - Validation Loss: 0.43, Validation Accuracy: 81.82%\n",
    "\n",
    "### Test Set Performance: ###\n",
    "The model demonstrates promising results on the test set, emphasizing robustness and generalization.\n",
    "\n",
    "- **Test Loss:** 0.43, **Test Accuracy:** 81.82%\n",
    "\n",
    "### Classification Report: ###\n",
    "The classification report provides a detailed breakdown of the model's performance across makeup product categories.\n",
    "\n",
    "- **Key Observations:**\n",
    "  - Precision, recall, and F1-score metrics exhibit improvements, especially for classes like \"Honey Love,\" \"Ruby Woo,\" and \"Whirl.\"\n",
    "  - The overall accuracy is 81.82%, highlighting the model's capability to correctly classify makeup products.\n",
    "\n",
    "### Conclusion: ###\n",
    "The new model results reflect substantial enhancements, emphasizing improved accuracy and class-specific metrics. This suggests that the model has successfully learned intricate patterns within the data, showcasing its potential for accurate lipstick shade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e6c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/shadesense_final_model4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/shadesense_final_model4/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the native Keras format\n",
    "model.save('models/shadesense_final_model4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f223e",
   "metadata": {},
   "source": [
    "## Conclusion and Future Steps ##\n",
    "\n",
    "The culmination of our model tuning efforts has resulted in a highly promising Convolutional Neural Network (CNN) based on VGG16. Leveraging strategies such as L2 regularization, dropout layers, and a custom learning rate, our model exhibits exceptional performance with a test accuracy of 81.82%. The classification report further emphasizes improved precision, recall, and F1-score metrics across various lipstick shades.\n",
    "\n",
    "Moving forward, we plan to integrate this finalized model into our ShadeSense Lipstick Shade Identifier App. Real-world usage will provide valuable insights into the model's practical performance, allowing us to analyze its strengths and identify areas for further refinement. Additionally, we remain committed to continuous improvement by fine-tuning the model, expanding our dataset, and exploring opportunities to enhance its capabilities.\n",
    "\n",
    "Excitement surrounds the prospect of observing the model's progression in real-world scenarios and its adaptability to a diverse range of user-generated inputs. This iterative approach ensures that our app evolves with user interactions, delivering accurate and reliable results for lipstick shade identification. We look forward to the app's continued development, with a keen eye on user feedback and data augmentation to propel its performance to new heights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
